{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_original  = pd.read_csv('Churn_Modelling.csv')\n",
    "df_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CreditScore Geography  Age  Tenure    Balance  NumOfProducts  HasCrCard  \\\n",
       "0          619    France   42       2       0.00              1          1   \n",
       "1          608     Spain   41       1   83807.86              1          0   \n",
       "2          502    France   42       8  159660.80              3          1   \n",
       "3          699    France   39       1       0.00              2          0   \n",
       "4          850     Spain   43       2  125510.82              1          1   \n",
       "\n",
       "   IsActiveMember  EstimatedSalary  Exited  \n",
       "0               1        101348.88       1  \n",
       "1               1        112542.58       0  \n",
       "2               0        113931.57       1  \n",
       "3               0         93826.63       0  \n",
       "4               1         79084.10       0  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_df = df_original.drop(columns=['RowNumber', 'CustomerId', 'Surname', 'Gender'])\n",
    "churn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "      <th>Geography_Germany</th>\n",
       "      <th>Geography_Spain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CreditScore  Age  Tenure    Balance  NumOfProducts  HasCrCard  \\\n",
       "0          619   42       2       0.00              1          1   \n",
       "1          608   41       1   83807.86              1          0   \n",
       "2          502   42       8  159660.80              3          1   \n",
       "3          699   39       1       0.00              2          0   \n",
       "4          850   43       2  125510.82              1          1   \n",
       "\n",
       "   IsActiveMember  EstimatedSalary  Exited  Geography_Germany  Geography_Spain  \n",
       "0               1        101348.88       1                  0                0  \n",
       "1               1        112542.58       0                  0                1  \n",
       "2               0        113931.57       1                  0                0  \n",
       "3               0         93826.63       0                  0                0  \n",
       "4               1         79084.10       0                  0                1  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_df2 = pd.get_dummies(churn_df, columns=['Geography'], drop_first=True, dtype=int)\n",
    "churn_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = churn_df2.drop('Exited', axis=1)\n",
    "y= churn_df2['Exited']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25 , stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3.1\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "900 fits failed out of a total of 2700.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "900 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\User\\anaconda3.1\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\User\\anaconda3.1\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 340, in fit\n",
      "    self._validate_params()\n",
      "  File \"c:\\Users\\User\\anaconda3.1\\Lib\\site-packages\\sklearn\\base.py\", line 600, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\User\\anaconda3.1\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'min_samples_split' parameter of RandomForestClassifier must be an int in the range [2, inf) or a float in the range (0.0, 1.0]. Got 1 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\User\\anaconda3.1\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan 1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      "        nan        nan        nan        nan 1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      "        nan        nan        nan        nan 1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      "        nan        nan        nan        nan 1.         1.\n",
      " 0.97571214 0.9826087  1.         1.         0.97571214 0.9826087\n",
      "        nan        nan        nan        nan 1.         1.\n",
      " 0.97571214 0.9826087  1.         1.         0.97571214 0.9826087\n",
      "        nan        nan        nan        nan 1.         1.\n",
      " 0.97571214 0.9826087  1.         1.         0.97571214 0.9826087\n",
      "        nan        nan        nan        nan 0.99130435 0.95675257\n",
      " 0.95439836 0.94181578 0.99130435 0.95675257 0.95439836 0.94181578\n",
      "        nan        nan        nan        nan 0.99130435 0.95708862\n",
      " 0.9541184  0.94138567 0.99130435 0.95708862 0.9541184  0.94138567\n",
      "        nan        nan        nan        nan 0.99130435 0.95708862\n",
      " 0.95426065 0.94152792 0.99130435 0.95708862 0.95426065 0.94152792\n",
      "        nan        nan        nan        nan 0.99047619 0.97042607\n",
      " 0.97685185 0.97916667 0.99047619 0.97042607 0.97685185 0.97916667\n",
      "        nan        nan        nan        nan 0.99047619 0.97042607\n",
      " 0.97685185 0.97916667 0.99047619 0.97042607 0.97685185 0.97916667\n",
      "        nan        nan        nan        nan 0.99090909 0.98181818\n",
      " 0.97685185 0.97909677 0.99090909 0.98181818 0.97685185 0.97909677\n",
      "        nan        nan        nan        nan 0.917865   0.91229657\n",
      " 0.90773219 0.90950776 0.91056065 0.91268304 0.90773219 0.90982148\n",
      "        nan        nan        nan        nan 0.91017702 0.91220452\n",
      " 0.91136993 0.90982148 0.91017702 0.91220452 0.91136993 0.90982148\n",
      "        nan        nan        nan        nan 0.90923083 0.91113448\n",
      " 0.91171007 0.90922823 0.90923083 0.91113448 0.91171007 0.90922823\n",
      "        nan        nan        nan        nan 0.8565421  0.83620578\n",
      " 0.8505854  0.84962499 0.8565421  0.83620578 0.8505854  0.84962499\n",
      "        nan        nan        nan        nan 0.85737466 0.83620578\n",
      " 0.8505854  0.85221047 0.85737466 0.83620578 0.8505854  0.85221047\n",
      "        nan        nan        nan        nan 0.85721822 0.83580974\n",
      " 0.8505854  0.85381772 0.85721822 0.83580974 0.8505854  0.85381772\n",
      "        nan        nan        nan        nan 0.92279379 0.92414904\n",
      " 0.93087731 0.93408452 0.92279379 0.92421187 0.93080567 0.93433503\n",
      "        nan        nan        nan        nan 0.91931784 0.92843804\n",
      " 0.93069884 0.93304919 0.91931784 0.92843804 0.93069884 0.93304919\n",
      "        nan        nan        nan        nan 0.92862504 0.92743663\n",
      " 0.93150167 0.92647224 0.92862504 0.92743663 0.93150167 0.92647224\n",
      "        nan        nan        nan        nan 0.84323303 0.83566176\n",
      " 0.837012   0.84615636 0.84307342 0.83374801 0.83410126 0.84656193\n",
      "        nan        nan        nan        nan 0.83869115 0.83783543\n",
      " 0.83917593 0.84301502 0.83869115 0.83783543 0.83917593 0.84301502\n",
      "        nan        nan        nan        nan 0.84417495 0.8450525\n",
      " 0.84218563 0.8499312  0.84417495 0.8450525  0.84218563 0.8499312\n",
      "        nan        nan        nan        nan 0.80824076 0.81631071\n",
      " 0.81560768 0.81424194 0.80824076 0.81631071 0.81560768 0.81528493\n",
      "        nan        nan        nan        nan 0.80928539 0.81895229\n",
      " 0.81740687 0.81451011 0.80928539 0.81895229 0.81740687 0.81451011\n",
      "        nan        nan        nan        nan 0.80997193 0.82048409\n",
      " 0.81788516 0.81502996 0.80997193 0.82048409 0.81788516 0.81502996\n",
      "        nan        nan        nan        nan 0.89949442 0.89593633\n",
      " 0.88582169 0.8834585  0.89478126 0.89501078 0.88619192 0.88420632\n",
      "        nan        nan        nan        nan 0.89630498 0.89574939\n",
      " 0.89489832 0.88369931 0.89630498 0.89574939 0.89489832 0.88369931\n",
      "        nan        nan        nan        nan 0.88888418 0.8949774\n",
      " 0.88095037 0.86669849 0.88888418 0.8949774  0.88095037 0.86669849\n",
      "        nan        nan        nan        nan 0.83247624 0.83914666\n",
      " 0.82807673 0.82835515 0.83257288 0.83708255 0.833121   0.82796003\n",
      "        nan        nan        nan        nan 0.83128848 0.83431763\n",
      " 0.82974381 0.82908095 0.83128848 0.83431763 0.82974381 0.82908095\n",
      "        nan        nan        nan        nan 0.82980379 0.83514931\n",
      " 0.82678402 0.82991805 0.82980379 0.83514931 0.82678402 0.82991805\n",
      "        nan        nan        nan        nan 0.81289867 0.79454009\n",
      " 0.79771558 0.79593803 0.81367838 0.79754678 0.80025567 0.79997445\n",
      "        nan        nan        nan        nan 0.80655423 0.79602815\n",
      " 0.7975899  0.79705588 0.80655423 0.79602815 0.7975899  0.79705588\n",
      "        nan        nan        nan        nan 0.80322944 0.79876822\n",
      " 0.79876434 0.79576391 0.80322944 0.79876822 0.79876434 0.79576391\n",
      "        nan        nan        nan        nan 0.76079129 0.76647102\n",
      " 0.76150756 0.76291569 0.77027344 0.77688034 0.77300047 0.77421415\n",
      "        nan        nan        nan        nan 0.77823118 0.78232311\n",
      " 0.78448506 0.78523775 0.77823118 0.78232311 0.78448506 0.78523775\n",
      "        nan        nan        nan        nan 0.78003289 0.77928784\n",
      " 0.77931766 0.77827331 0.78003289 0.77928784 0.77931766 0.77827331\n",
      "        nan        nan        nan        nan 0.73720407 0.74310948\n",
      " 0.74951492 0.7588934  0.75265376 0.75129267 0.7489156  0.75477344\n",
      "        nan        nan        nan        nan 0.75534096 0.76333623\n",
      " 0.76973591 0.76268356 0.75534096 0.76333623 0.76973591 0.76268356\n",
      "        nan        nan        nan        nan 0.76889638 0.77203507\n",
      " 0.77449935 0.77319944 0.76889638 0.77203507 0.77449935 0.77319944\n",
      "        nan        nan        nan        nan 0.72961741 0.73473353\n",
      " 0.73593008 0.73599187 0.74175456 0.74085864 0.73862822 0.74248069\n",
      "        nan        nan        nan        nan 0.74866886 0.75326077\n",
      " 0.75628905 0.75483868 0.74866886 0.75326077 0.75628905 0.75483868\n",
      "        nan        nan        nan        nan 0.76677988 0.76120236\n",
      " 0.76171124 0.7610645  0.76677988 0.76120236 0.76171124 0.7610645 ]\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\anaconda3.1\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan 0.01898211 0.02749598\n",
      " 0.03338262 0.03927569 0.01898211 0.02749598 0.03338262 0.03927569\n",
      "        nan        nan        nan        nan 0.01898211 0.02749598\n",
      " 0.03338262 0.03927569 0.01898211 0.02749598 0.03338262 0.03927569\n",
      "        nan        nan        nan        nan 0.01963784 0.02749598\n",
      " 0.03338262 0.03927569 0.01963784 0.02749598 0.03338262 0.03927569\n",
      "        nan        nan        nan        nan 0.05434051 0.06284367\n",
      " 0.06939034 0.07004607 0.05434051 0.06284367 0.06939034 0.07004607\n",
      "        nan        nan        nan        nan 0.05434051 0.06284367\n",
      " 0.06939034 0.07004607 0.05434051 0.06284367 0.06939034 0.07004607\n",
      "        nan        nan        nan        nan 0.05368692 0.06284367\n",
      " 0.06939034 0.07004607 0.05368692 0.06284367 0.06939034 0.07004607\n",
      "        nan        nan        nan        nan 0.07461695 0.09555984\n",
      " 0.1066881  0.10407157 0.07461695 0.09555984 0.1066881  0.10407157\n",
      "        nan        nan        nan        nan 0.07461695 0.09555984\n",
      " 0.10669024 0.10341798 0.07461695 0.09555984 0.10669024 0.10341798\n",
      "        nan        nan        nan        nan 0.07461695 0.09555984\n",
      " 0.10734598 0.10407372 0.07461695 0.09555984 0.10734598 0.10407372\n",
      "        nan        nan        nan        nan 0.06153005 0.06415086\n",
      " 0.07527483 0.08181721 0.06153005 0.06415086 0.07527483 0.08181721\n",
      "        nan        nan        nan        nan 0.06153005 0.06349727\n",
      " 0.07527483 0.08116147 0.06153005 0.06349727 0.07527483 0.08116147\n",
      "        nan        nan        nan        nan 0.06218365 0.0654602\n",
      " 0.07593057 0.08247295 0.06218365 0.0654602  0.07593057 0.08247295\n",
      "        nan        nan        nan        nan 0.18196507 0.18262509\n",
      " 0.19113683 0.19178614 0.18327226 0.18327869 0.19113683 0.19243973\n",
      "        nan        nan        nan        nan 0.18261866 0.18262724\n",
      " 0.19244616 0.19243973 0.18261866 0.18262724 0.19244616 0.19243973\n",
      "        nan        nan        nan        nan 0.18065788 0.18066645\n",
      " 0.19309975 0.19113254 0.18065788 0.18066645 0.19309975 0.19113254\n",
      "        nan        nan        nan        nan 0.23039109 0.24020786\n",
      " 0.23235616 0.23693989 0.23039109 0.24020786 0.23235616 0.23693989\n",
      "        nan        nan        nan        nan 0.23169827 0.24020786\n",
      " 0.23235616 0.23824708 0.23169827 0.24020786 0.23235616 0.23824708\n",
      "        nan        nan        nan        nan 0.23170042 0.23955427\n",
      " 0.23235616 0.23759134 0.23170042 0.23955427 0.23235616 0.23759134\n",
      "        nan        nan        nan        nan 0.17148398 0.17410908\n",
      " 0.17608057 0.18066645 0.17148398 0.17410693 0.17608057 0.18132005\n",
      "        nan        nan        nan        nan 0.17148184 0.17542055\n",
      " 0.17607843 0.18394086 0.17148184 0.17542055 0.17607843 0.18394086\n",
      "        nan        nan        nan        nan 0.17017251 0.17345334\n",
      " 0.1786971  0.18066645 0.17017251 0.17345334 0.1786971  0.18066645\n",
      "        nan        nan        nan        nan 0.2572399  0.2552834\n",
      " 0.26641594 0.26838101 0.25724204 0.25593914 0.26641594 0.26903461\n",
      "        nan        nan        nan        nan 0.25788921 0.25528126\n",
      " 0.26575806 0.2703418  0.25788921 0.25528126 0.26575806 0.2703418\n",
      "        nan        nan        nan        nan 0.25657988 0.25659274\n",
      " 0.26314583 0.26903889 0.25657988 0.25659274 0.26314583 0.26903889\n",
      "        nan        nan        nan        nan 0.34820101 0.34492446\n",
      " 0.34295511 0.34164792 0.34820101 0.34492446 0.34295511 0.34164792\n",
      "        nan        nan        nan        nan 0.34754741 0.34427087\n",
      " 0.34033644 0.33903139 0.34754741 0.34427087 0.34033644 0.33903139\n",
      "        nan        nan        nan        nan 0.34689596 0.34296582\n",
      " 0.33902711 0.33771992 0.34689596 0.34296582 0.33902711 0.33771992\n",
      "        nan        nan        nan        nan 0.21337405 0.22123004\n",
      " 0.22646523 0.23563484 0.2140255  0.21992285 0.22711883 0.23759349\n",
      "        nan        nan        nan        nan 0.21010822 0.21861566\n",
      " 0.22777671 0.23759563 0.21010822 0.21861566 0.22777671 0.23759563\n",
      "        nan        nan        nan        nan 0.20748312 0.21860924\n",
      " 0.22777456 0.23825137 0.20748312 0.21860924 0.22777456 0.23825137\n",
      "        nan        nan        nan        nan 0.33706418 0.34099646\n",
      " 0.34099432 0.34099218 0.33706632 0.34165006 0.34361299 0.34230151\n",
      "        nan        nan        nan        nan 0.3396807  0.33771992\n",
      " 0.34164577 0.34229937 0.3396807  0.33771992 0.34164577 0.34229937\n",
      "        nan        nan        nan        nan 0.33706418 0.33576342\n",
      " 0.33706847 0.33968499 0.33706418 0.33576342 0.33706847 0.33968499\n",
      "        nan        nan        nan        nan 0.36391085 0.37307832\n",
      " 0.37307832 0.37700204 0.36653595 0.37308261 0.37308047 0.37831137\n",
      "        nan        nan        nan        nan 0.36587592 0.37242044\n",
      " 0.37373192 0.37765992 0.36587592 0.37242044 0.37373192 0.37765992\n",
      "        nan        nan        nan        nan 0.36653595 0.37896496\n",
      " 0.37569913 0.3756927  0.36653595 0.37896496 0.37569913 0.3756927\n",
      "        nan        nan        nan        nan 0.43588128 0.43457195\n",
      " 0.44046073 0.43522769 0.44832101 0.44176792 0.44242366 0.43915354\n",
      "        nan        nan        nan        nan 0.43716918 0.44240866\n",
      " 0.44568306 0.44503375 0.43716918 0.44240866 0.44568306 0.44503375\n",
      "        nan        nan        nan        nan 0.43065038 0.42933676\n",
      " 0.42868317 0.42672453 0.43065038 0.42933676 0.42868317 0.42672453\n",
      "        nan        nan        nan        nan 0.45223615 0.45288546\n",
      " 0.46270867 0.46532519 0.45747991 0.4555127  0.45617058 0.46139934\n",
      "        nan        nan        nan        nan 0.45812493 0.4627001\n",
      " 0.46597878 0.46008572 0.45812493 0.4627001  0.46597878 0.46008572\n",
      "        nan        nan        nan        nan 0.45943427 0.46009\n",
      " 0.46139719 0.4574692  0.45943427 0.46009    0.46139719 0.4574692\n",
      "        nan        nan        nan        nan 0.4744798  0.46270867\n",
      " 0.47318119 0.46729455 0.46925104 0.47056895 0.47580628 0.47121397\n",
      "        nan        nan        nan        nan 0.46335155 0.47054538\n",
      " 0.47251688 0.47120969 0.46335155 0.47054538 0.47251688 0.47120969\n",
      "        nan        nan        nan        nan 0.46531662 0.45943212\n",
      " 0.46270438 0.46336012 0.46531662 0.45943212 0.46270438 0.46336012]\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\anaconda3.1\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan 0.03722925 0.05342759\n",
      " 0.06449549 0.0753663  0.03722925 0.05342759 0.06449549 0.0753663\n",
      "        nan        nan        nan        nan 0.03722925 0.05342759\n",
      " 0.06449549 0.0753663  0.03722925 0.05342759 0.06449549 0.0753663\n",
      "        nan        nan        nan        nan 0.03848656 0.05342759\n",
      " 0.06449549 0.0753663  0.03848656 0.05342759 0.06449549 0.0753663\n",
      "        nan        nan        nan        nan 0.10263895 0.11801435\n",
      " 0.12915351 0.13040494 0.10263895 0.11801435 0.12915351 0.13040494\n",
      "        nan        nan        nan        nan 0.10263895 0.11801435\n",
      " 0.12915351 0.13040494 0.10263895 0.11801435 0.12915351 0.13040494\n",
      "        nan        nan        nan        nan 0.10146936 0.11801435\n",
      " 0.12915351 0.13040494 0.10146936 0.11801435 0.12915351 0.13040494\n",
      "        nan        nan        nan        nan 0.1386058  0.1735242\n",
      " 0.19165965 0.18722862 0.1386058  0.1735242  0.19165965 0.18722862\n",
      "        nan        nan        nan        nan 0.1386058  0.17347717\n",
      " 0.19162116 0.18614059 0.1386058  0.17347717 0.19162116 0.18614059\n",
      "        nan        nan        nan        nan 0.1386058  0.17347717\n",
      " 0.19266459 0.18718401 0.1386058  0.17347717 0.19266459 0.18718401\n",
      "        nan        nan        nan        nan 0.11562197 0.12018459\n",
      " 0.13943356 0.15058486 0.11562197 0.12018459 0.13943356 0.15058486\n",
      "        nan        nan        nan        nan 0.11562197 0.118993\n",
      " 0.13943356 0.14946795 0.11562197 0.118993   0.13943356 0.14946795\n",
      "        nan        nan        nan        nan 0.1167669  0.12253217\n",
      " 0.14053707 0.15171785 0.1167669  0.12253217 0.14053707 0.15171785\n",
      "        nan        nan        nan        nan 0.30228182 0.30264704\n",
      " 0.3141031  0.31505914 0.30396721 0.30365067 0.3141031  0.31603481\n",
      "        nan        nan        nan        nan 0.30306882 0.30270382\n",
      " 0.31599677 0.31603481 0.30306882 0.30270382 0.31599677 0.31603481\n",
      "        nan        nan        nan        nan 0.30014148 0.29973138\n",
      " 0.31698348 0.31409968 0.30014148 0.29973138 0.31698348 0.31409968\n",
      "        nan        nan        nan        nan 0.36267064 0.37245035\n",
      " 0.36435742 0.3698037  0.36267064 0.37245035 0.36435742 0.3698037\n",
      "        nan        nan        nan        nan 0.36429878 0.37245035\n",
      " 0.36435742 0.37160666 0.36429878 0.37245035 0.36435742 0.37160666\n",
      "        nan        nan        nan        nan 0.36430048 0.37166121\n",
      " 0.36435742 0.37101872 0.36430048 0.37166121 0.36435742 0.37101872\n",
      "        nan        nan        nan        nan 0.28736803 0.2911046\n",
      " 0.29383263 0.30076944 0.28736803 0.29118141 0.29389123 0.30179034\n",
      "        nan        nan        nan        nan 0.28751063 0.29293199\n",
      " 0.29407977 0.3056246  0.28751063 0.29293199 0.29407977 0.3056246\n",
      "        nan        nan        nan        nan 0.28593214 0.29029455\n",
      " 0.29782319 0.30053752 0.28593214 0.29029455 0.29782319 0.30053752\n",
      "        nan        nan        nan        nan 0.39286454 0.38976824\n",
      " 0.40226463 0.40510473 0.39279412 0.39033559 0.40179672 0.40587814\n",
      "        nan        nan        nan        nan 0.39306551 0.39003284\n",
      " 0.40159135 0.40722605 0.39306551 0.39003284 0.40159135 0.40722605\n",
      "        nan        nan        nan        nan 0.39248292 0.39231995\n",
      " 0.39906415 0.40654754 0.39248292 0.39231995 0.39906415 0.40654754\n",
      "        nan        nan        nan        nan 0.48534649 0.48390701\n",
      " 0.48198239 0.48032843 0.48534649 0.48390701 0.48198239 0.4805521\n",
      "        nan        nan        nan        nan 0.48494047 0.48364277\n",
      " 0.47961063 0.47771978 0.48494047 0.48364277 0.47961063 0.47771978\n",
      "        nan        nan        nan        nan 0.48443224 0.48256115\n",
      " 0.47841318 0.47657047 0.48443224 0.48256115 0.47841318 0.47657047\n",
      "        nan        nan        nan        nan 0.34404367 0.35400383\n",
      " 0.36007112 0.37116712 0.34471132 0.35229792 0.36094877 0.37385503\n",
      "        nan        nan        nan        nan 0.3394484  0.35035098\n",
      " 0.36226037 0.37332189 0.3394484  0.35035098 0.36226037 0.37332189\n",
      "        nan        nan        nan        nan 0.33572078 0.35053626\n",
      " 0.36103623 0.37266757 0.33572078 0.35053626 0.36103623 0.37266757\n",
      "        nan        nan        nan        nan 0.47926502 0.48419708\n",
      " 0.48228602 0.48234381 0.47918924 0.4844969  0.48572897 0.48339783\n",
      "        nan        nan        nan        nan 0.48163347 0.4801799\n",
      " 0.48321648 0.48371601 0.48163347 0.4801799  0.48321648 0.48371601\n",
      "        nan        nan        nan        nan 0.47882454 0.47839652\n",
      " 0.4782478  0.48132708 0.47882454 0.47839652 0.4782478  0.48132708\n",
      "        nan        nan        nan        nan 0.50170936 0.50658235\n",
      " 0.50681271 0.5101833  0.50425054 0.50704717 0.50723419 0.51212654\n",
      "        nan        nan        nan        nan 0.50260761 0.50637665\n",
      " 0.50777283 0.51130459 0.50260761 0.50637665 0.50777283 0.51130459\n",
      "        nan        nan        nan        nan 0.50189419 0.51245594\n",
      " 0.50906105 0.50845824 0.50189419 0.51245594 0.50906105 0.50845824\n",
      "        nan        nan        nan        nan 0.55295294 0.55347798\n",
      " 0.55709017 0.55336793 0.56609145 0.56234416 0.56156116 0.55929152\n",
      "        nan        nan        nan        nan 0.55915342 0.56427466\n",
      " 0.56744322 0.56737586 0.55915342 0.56427466 0.56744322 0.56737586\n",
      "        nan        nan        nan        nan 0.55426032 0.55292453\n",
      " 0.55231006 0.5504428  0.55426032 0.55292453 0.55231006 0.5504428\n",
      "        nan        nan        nan        nan 0.55915487 0.5617127\n",
      " 0.57118515 0.57575204 0.56820306 0.56617504 0.56619136 0.57170346\n",
      "        nan        nan        nan        nan 0.56916667 0.5749996\n",
      " 0.57933546 0.57276709 0.56916667 0.5749996  0.57933546 0.57276709\n",
      "        nan        nan        nan        nan 0.57417301 0.5753051\n",
      " 0.5770875  0.57399638 0.57417301 0.5753051  0.5770875  0.57399638\n",
      "        nan        nan        nan        nan 0.57359592 0.56658011\n",
      " 0.57488514 0.57035623 0.57376621 0.57440285 0.57757834 0.57530348\n",
      "        nan        nan        nan        nan 0.57112953 0.57770484\n",
      " 0.58052856 0.57904801 0.57112953 0.57770484 0.58052856 0.57904801\n",
      "        nan        nan        nan        nan 0.57796669 0.57195662\n",
      " 0.57467237 0.5749074  0.57796669 0.57195662 0.57467237 0.5749074 ]\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\anaconda3.1\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan 0.80013333 0.80186667\n",
      " 0.80306667 0.80426667 0.80013333 0.80186667 0.80306667 0.80426667\n",
      "        nan        nan        nan        nan 0.80013333 0.80186667\n",
      " 0.80306667 0.80426667 0.80013333 0.80186667 0.80306667 0.80426667\n",
      "        nan        nan        nan        nan 0.80026667 0.80186667\n",
      " 0.80306667 0.80426667 0.80026667 0.80186667 0.80306667 0.80426667\n",
      "        nan        nan        nan        nan 0.80733333 0.80906667\n",
      " 0.81       0.81026667 0.80733333 0.80906667 0.81       0.81026667\n",
      "        nan        nan        nan        nan 0.80733333 0.80906667\n",
      " 0.81       0.81026667 0.80733333 0.80906667 0.81       0.81026667\n",
      "        nan        nan        nan        nan 0.8072     0.80906667\n",
      " 0.81       0.81026667 0.8072     0.80906667 0.81       0.81026667\n",
      "        nan        nan        nan        nan 0.81133333 0.8148\n",
      " 0.81693333 0.81613333 0.81133333 0.8148     0.81693333 0.81613333\n",
      "        nan        nan        nan        nan 0.81133333 0.8148\n",
      " 0.81693333 0.816      0.81133333 0.8148     0.81693333 0.816\n",
      "        nan        nan        nan        nan 0.81133333 0.8148\n",
      " 0.81706667 0.81613333 0.81133333 0.8148     0.81706667 0.81613333\n",
      "        nan        nan        nan        nan 0.80866667 0.80893333\n",
      " 0.8112     0.81253333 0.80866667 0.80893333 0.8112     0.81253333\n",
      "        nan        nan        nan        nan 0.80866667 0.8088\n",
      " 0.8112     0.8124     0.80866667 0.8088     0.8112     0.8124\n",
      "        nan        nan        nan        nan 0.8088     0.80933333\n",
      " 0.81133333 0.81266667 0.8088     0.80933333 0.81133333 0.81266667\n",
      "        nan        nan        nan        nan 0.82973333 0.82973333\n",
      " 0.83106667 0.8312     0.82973333 0.82986667 0.83106667 0.83133333\n",
      "        nan        nan        nan        nan 0.8296     0.82973333\n",
      " 0.83146667 0.83133333 0.8296     0.82973333 0.83146667 0.83133333\n",
      "        nan        nan        nan        nan 0.8292     0.82933333\n",
      " 0.8316     0.83106667 0.8292     0.82933333 0.8316     0.83106667\n",
      "        nan        nan        nan        nan 0.83533333 0.8356\n",
      " 0.83533333 0.836      0.83533333 0.8356     0.83533333 0.836\n",
      "        nan        nan        nan        nan 0.8356     0.8356\n",
      " 0.83533333 0.8364     0.8356     0.8356     0.83533333 0.8364\n",
      "        nan        nan        nan        nan 0.8356     0.83546667\n",
      " 0.83533333 0.8364     0.8356     0.83546667 0.83533333 0.8364\n",
      "        nan        nan        nan        nan 0.828      0.82853333\n",
      " 0.8292     0.83026667 0.828      0.82853333 0.8292     0.8304\n",
      "        nan        nan        nan        nan 0.82786667 0.82893333\n",
      " 0.8292     0.83093333 0.82786667 0.82893333 0.8292     0.83093333\n",
      "        nan        nan        nan        nan 0.828      0.82853333\n",
      " 0.82973333 0.83       0.828      0.82853333 0.82973333 0.83\n",
      "        nan        nan        nan        nan 0.83893333 0.83813333\n",
      " 0.84       0.8408     0.83893333 0.83813333 0.83973333 0.84093333\n",
      "        nan        nan        nan        nan 0.83866667 0.83826667\n",
      " 0.84       0.84106667 0.83866667 0.83826667 0.84       0.84106667\n",
      "        nan        nan        nan        nan 0.83893333 0.83906667\n",
      " 0.83986667 0.84133333 0.83893333 0.83906667 0.83986667 0.84133333\n",
      "        nan        nan        nan        nan 0.85       0.8504\n",
      " 0.85       0.8496     0.85       0.8504     0.85       0.84973333\n",
      "        nan        nan        nan        nan 0.85       0.85053333\n",
      " 0.84973333 0.8492     0.85       0.85053333 0.84973333 0.8492\n",
      "        nan        nan        nan        nan 0.85       0.85053333\n",
      " 0.8496     0.84906667 0.85       0.85053333 0.8496     0.84906667\n",
      "        nan        nan        nan        nan 0.8348     0.836\n",
      " 0.8364     0.83786667 0.83466667 0.83573333 0.83653333 0.83826667\n",
      "        nan        nan        nan        nan 0.834      0.83546667\n",
      " 0.83706667 0.83813333 0.834      0.83546667 0.83706667 0.83813333\n",
      "        nan        nan        nan        nan 0.8332     0.83546667\n",
      " 0.83626667 0.8372     0.8332     0.83546667 0.83626667 0.8372\n",
      "        nan        nan        nan        nan 0.85093333 0.85226667\n",
      " 0.85106667 0.85106667 0.85093333 0.85213333 0.852      0.8512\n",
      "        nan        nan        nan        nan 0.8512     0.8512\n",
      " 0.85133333 0.85133333 0.8512     0.8512     0.85133333 0.85133333\n",
      "        nan        nan        nan        nan 0.85066667 0.85106667\n",
      " 0.8504     0.85106667 0.85066667 0.85106667 0.8504     0.85106667\n",
      "        nan        nan        nan        nan 0.85293333 0.85213333\n",
      " 0.85253333 0.85293333 0.85346667 0.85253333 0.8528     0.8536\n",
      "        nan        nan        nan        nan 0.85266667 0.85226667\n",
      " 0.85266667 0.8532     0.85266667 0.85226667 0.85266667 0.8532\n",
      "        nan        nan        nan        nan 0.85213333 0.85346667\n",
      " 0.85293333 0.85253333 0.85213333 0.85346667 0.85293333 0.85253333\n",
      "        nan        nan        nan        nan 0.85666667 0.85746667\n",
      " 0.8576     0.85706667 0.86013333 0.86013333 0.8596     0.85933333\n",
      "        nan        nan        nan        nan 0.8596     0.86093333\n",
      " 0.86173333 0.86186667 0.8596     0.86093333 0.86173333 0.86186667\n",
      "        nan        nan        nan        nan 0.85906667 0.8588\n",
      " 0.85866667 0.85826667 0.85906667 0.8588     0.85866667 0.85826667\n",
      "        nan        nan        nan        nan 0.85506667 0.85626667\n",
      " 0.85866667 0.86053333 0.85866667 0.85813333 0.85786667 0.85946667\n",
      "        nan        nan        nan        nan 0.8588     0.8608\n",
      " 0.8624     0.8604     0.8588     0.8608     0.8624     0.8604\n",
      "        nan        nan        nan        nan 0.8616     0.862\n",
      " 0.86253333 0.86186667 0.8616     0.862      0.86253333 0.86186667\n",
      "        nan        nan        nan        nan 0.85666667 0.856\n",
      " 0.8576     0.8568     0.85813333 0.85813333 0.85853333 0.85853333\n",
      "        nan        nan        nan        nan 0.85866667 0.8604\n",
      " 0.86133333 0.86093333 0.85866667 0.8604     0.86133333 0.86093333\n",
      "        nan        nan        nan        nan 0.86173333 0.86\n",
      " 0.86066667 0.86066667 0.86173333 0.86       0.86066667 0.86066667]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 57s\n",
      "Wall time: 13min 23s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=0),\n",
       "             param_grid={&#x27;max_depth&#x27;: [2, 3, 4, 5, None],\n",
       "                         &#x27;max_features&#x27;: [2, 3, 4],\n",
       "                         &#x27;min_samples_leaf&#x27;: [1, 2, 3],\n",
       "                         &#x27;min_samples_split&#x27;: [1, 2, 3],\n",
       "                         &#x27;n_estimators&#x27;: [75, 100, 125, 150]},\n",
       "             refit=&#x27;f1&#x27;, scoring={&#x27;precision&#x27;, &#x27;recall&#x27;, &#x27;f1&#x27;, &#x27;accuracy&#x27;})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=0),\n",
       "             param_grid={&#x27;max_depth&#x27;: [2, 3, 4, 5, None],\n",
       "                         &#x27;max_features&#x27;: [2, 3, 4],\n",
       "                         &#x27;min_samples_leaf&#x27;: [1, 2, 3],\n",
       "                         &#x27;min_samples_split&#x27;: [1, 2, 3],\n",
       "                         &#x27;n_estimators&#x27;: [75, 100, 125, 150]},\n",
       "             refit=&#x27;f1&#x27;, scoring={&#x27;precision&#x27;, &#x27;recall&#x27;, &#x27;f1&#x27;, &#x27;accuracy&#x27;})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=0)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=0)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(random_state=0),\n",
       "             param_grid={'max_depth': [2, 3, 4, 5, None],\n",
       "                         'max_features': [2, 3, 4],\n",
       "                         'min_samples_leaf': [1, 2, 3],\n",
       "                         'min_samples_split': [1, 2, 3],\n",
       "                         'n_estimators': [75, 100, 125, 150]},\n",
       "             refit='f1', scoring={'precision', 'recall', 'f1', 'accuracy'})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "cv_params = {'max_depth':[2,3,4,5, None],\n",
    "               'min_samples_leaf':[1,2,3],\n",
    "               'min_samples_split':[1,2,3],\n",
    "               'max_features': [2,3,4],\n",
    "               'n_estimators': [75, 100, 125, 150]} \n",
    "\n",
    "scoring = {'accuracy', 'precision', 'recall', 'f1'}\n",
    "\n",
    "rf_cv = GridSearchCV(rf ,param_grid=cv_params, scoring=scoring ,cv=5, refit='f1')\n",
    "\n",
    "rf_cv.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\To lenovo\\Baramej_Courses\\coursera\\02 Google Advanced Data Analytics\\Activities'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the model\n",
    "with open(path + 'rf_cv_model_p.pickle', 'wb') as to_write:\n",
    "    pickle.dump(rf_cv, to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open pickled model\n",
    "with open(path + 'rf_cv_model_p.pickle', 'rb') as to_read:\n",
    "    rf_cv = pickle.load(to_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3.1\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "900 fits failed out of a total of 2700.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "900 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\User\\anaconda3.1\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\User\\anaconda3.1\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 340, in fit\n",
      "    self._validate_params()\n",
      "  File \"c:\\Users\\User\\anaconda3.1\\Lib\\site-packages\\sklearn\\base.py\", line 600, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\User\\anaconda3.1\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'min_samples_split' parameter of RandomForestClassifier must be an int in the range [2, inf) or a float in the range (0.0, 1.0]. Got 1 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\User\\anaconda3.1\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan 1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      "        nan        nan        nan        nan 1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      "        nan        nan        nan        nan 1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      "        nan        nan        nan        nan 1.         1.\n",
      " 0.97571214 0.9826087  1.         1.         0.97571214 0.9826087\n",
      "        nan        nan        nan        nan 1.         1.\n",
      " 0.97571214 0.9826087  1.         1.         0.97571214 0.9826087\n",
      "        nan        nan        nan        nan 1.         1.\n",
      " 0.97571214 0.9826087  1.         1.         0.97571214 0.9826087\n",
      "        nan        nan        nan        nan 0.99130435 0.95675257\n",
      " 0.95439836 0.94181578 0.99130435 0.95675257 0.95439836 0.94181578\n",
      "        nan        nan        nan        nan 0.99130435 0.95708862\n",
      " 0.9541184  0.94138567 0.99130435 0.95708862 0.9541184  0.94138567\n",
      "        nan        nan        nan        nan 0.99130435 0.95708862\n",
      " 0.95426065 0.94152792 0.99130435 0.95708862 0.95426065 0.94152792\n",
      "        nan        nan        nan        nan 0.99047619 0.97042607\n",
      " 0.97685185 0.97916667 0.99047619 0.97042607 0.97685185 0.97916667\n",
      "        nan        nan        nan        nan 0.99047619 0.97042607\n",
      " 0.97685185 0.97916667 0.99047619 0.97042607 0.97685185 0.97916667\n",
      "        nan        nan        nan        nan 0.99090909 0.98181818\n",
      " 0.97685185 0.97909677 0.99090909 0.98181818 0.97685185 0.97909677\n",
      "        nan        nan        nan        nan 0.917865   0.91229657\n",
      " 0.90773219 0.90950776 0.91056065 0.91268304 0.90773219 0.90982148\n",
      "        nan        nan        nan        nan 0.91017702 0.91220452\n",
      " 0.91136993 0.90982148 0.91017702 0.91220452 0.91136993 0.90982148\n",
      "        nan        nan        nan        nan 0.90923083 0.91113448\n",
      " 0.91171007 0.90922823 0.90923083 0.91113448 0.91171007 0.90922823\n",
      "        nan        nan        nan        nan 0.8565421  0.83620578\n",
      " 0.8505854  0.84962499 0.8565421  0.83620578 0.8505854  0.84962499\n",
      "        nan        nan        nan        nan 0.85737466 0.83620578\n",
      " 0.8505854  0.85221047 0.85737466 0.83620578 0.8505854  0.85221047\n",
      "        nan        nan        nan        nan 0.85721822 0.83580974\n",
      " 0.8505854  0.85381772 0.85721822 0.83580974 0.8505854  0.85381772\n",
      "        nan        nan        nan        nan 0.92279379 0.92414904\n",
      " 0.93087731 0.93408452 0.92279379 0.92421187 0.93080567 0.93433503\n",
      "        nan        nan        nan        nan 0.91931784 0.92843804\n",
      " 0.93069884 0.93304919 0.91931784 0.92843804 0.93069884 0.93304919\n",
      "        nan        nan        nan        nan 0.92862504 0.92743663\n",
      " 0.93150167 0.92647224 0.92862504 0.92743663 0.93150167 0.92647224\n",
      "        nan        nan        nan        nan 0.84323303 0.83566176\n",
      " 0.837012   0.84615636 0.84307342 0.83374801 0.83410126 0.84656193\n",
      "        nan        nan        nan        nan 0.83869115 0.83783543\n",
      " 0.83917593 0.84301502 0.83869115 0.83783543 0.83917593 0.84301502\n",
      "        nan        nan        nan        nan 0.84417495 0.8450525\n",
      " 0.84218563 0.8499312  0.84417495 0.8450525  0.84218563 0.8499312\n",
      "        nan        nan        nan        nan 0.80824076 0.81631071\n",
      " 0.81560768 0.81424194 0.80824076 0.81631071 0.81560768 0.81528493\n",
      "        nan        nan        nan        nan 0.80928539 0.81895229\n",
      " 0.81740687 0.81451011 0.80928539 0.81895229 0.81740687 0.81451011\n",
      "        nan        nan        nan        nan 0.80997193 0.82048409\n",
      " 0.81788516 0.81502996 0.80997193 0.82048409 0.81788516 0.81502996\n",
      "        nan        nan        nan        nan 0.89949442 0.89593633\n",
      " 0.88582169 0.8834585  0.89478126 0.89501078 0.88619192 0.88420632\n",
      "        nan        nan        nan        nan 0.89630498 0.89574939\n",
      " 0.89489832 0.88369931 0.89630498 0.89574939 0.89489832 0.88369931\n",
      "        nan        nan        nan        nan 0.88888418 0.8949774\n",
      " 0.88095037 0.86669849 0.88888418 0.8949774  0.88095037 0.86669849\n",
      "        nan        nan        nan        nan 0.83247624 0.83914666\n",
      " 0.82807673 0.82835515 0.83257288 0.83708255 0.833121   0.82796003\n",
      "        nan        nan        nan        nan 0.83128848 0.83431763\n",
      " 0.82974381 0.82908095 0.83128848 0.83431763 0.82974381 0.82908095\n",
      "        nan        nan        nan        nan 0.82980379 0.83514931\n",
      " 0.82678402 0.82991805 0.82980379 0.83514931 0.82678402 0.82991805\n",
      "        nan        nan        nan        nan 0.81289867 0.79454009\n",
      " 0.79771558 0.79593803 0.81367838 0.79754678 0.80025567 0.79997445\n",
      "        nan        nan        nan        nan 0.80655423 0.79602815\n",
      " 0.7975899  0.79705588 0.80655423 0.79602815 0.7975899  0.79705588\n",
      "        nan        nan        nan        nan 0.80322944 0.79876822\n",
      " 0.79876434 0.79576391 0.80322944 0.79876822 0.79876434 0.79576391\n",
      "        nan        nan        nan        nan 0.76079129 0.76647102\n",
      " 0.76150756 0.76291569 0.77027344 0.77688034 0.77300047 0.77421415\n",
      "        nan        nan        nan        nan 0.77823118 0.78232311\n",
      " 0.78448506 0.78523775 0.77823118 0.78232311 0.78448506 0.78523775\n",
      "        nan        nan        nan        nan 0.78003289 0.77928784\n",
      " 0.77931766 0.77827331 0.78003289 0.77928784 0.77931766 0.77827331\n",
      "        nan        nan        nan        nan 0.73720407 0.74310948\n",
      " 0.74951492 0.7588934  0.75265376 0.75129267 0.7489156  0.75477344\n",
      "        nan        nan        nan        nan 0.75534096 0.76333623\n",
      " 0.76973591 0.76268356 0.75534096 0.76333623 0.76973591 0.76268356\n",
      "        nan        nan        nan        nan 0.76889638 0.77203507\n",
      " 0.77449935 0.77319944 0.76889638 0.77203507 0.77449935 0.77319944\n",
      "        nan        nan        nan        nan 0.72961741 0.73473353\n",
      " 0.73593008 0.73599187 0.74175456 0.74085864 0.73862822 0.74248069\n",
      "        nan        nan        nan        nan 0.74866886 0.75326077\n",
      " 0.75628905 0.75483868 0.74866886 0.75326077 0.75628905 0.75483868\n",
      "        nan        nan        nan        nan 0.76677988 0.76120236\n",
      " 0.76171124 0.7610645  0.76677988 0.76120236 0.76171124 0.7610645 ]\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\anaconda3.1\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan 0.01898211 0.02749598\n",
      " 0.03338262 0.03927569 0.01898211 0.02749598 0.03338262 0.03927569\n",
      "        nan        nan        nan        nan 0.01898211 0.02749598\n",
      " 0.03338262 0.03927569 0.01898211 0.02749598 0.03338262 0.03927569\n",
      "        nan        nan        nan        nan 0.01963784 0.02749598\n",
      " 0.03338262 0.03927569 0.01963784 0.02749598 0.03338262 0.03927569\n",
      "        nan        nan        nan        nan 0.05434051 0.06284367\n",
      " 0.06939034 0.07004607 0.05434051 0.06284367 0.06939034 0.07004607\n",
      "        nan        nan        nan        nan 0.05434051 0.06284367\n",
      " 0.06939034 0.07004607 0.05434051 0.06284367 0.06939034 0.07004607\n",
      "        nan        nan        nan        nan 0.05368692 0.06284367\n",
      " 0.06939034 0.07004607 0.05368692 0.06284367 0.06939034 0.07004607\n",
      "        nan        nan        nan        nan 0.07461695 0.09555984\n",
      " 0.1066881  0.10407157 0.07461695 0.09555984 0.1066881  0.10407157\n",
      "        nan        nan        nan        nan 0.07461695 0.09555984\n",
      " 0.10669024 0.10341798 0.07461695 0.09555984 0.10669024 0.10341798\n",
      "        nan        nan        nan        nan 0.07461695 0.09555984\n",
      " 0.10734598 0.10407372 0.07461695 0.09555984 0.10734598 0.10407372\n",
      "        nan        nan        nan        nan 0.06153005 0.06415086\n",
      " 0.07527483 0.08181721 0.06153005 0.06415086 0.07527483 0.08181721\n",
      "        nan        nan        nan        nan 0.06153005 0.06349727\n",
      " 0.07527483 0.08116147 0.06153005 0.06349727 0.07527483 0.08116147\n",
      "        nan        nan        nan        nan 0.06218365 0.0654602\n",
      " 0.07593057 0.08247295 0.06218365 0.0654602  0.07593057 0.08247295\n",
      "        nan        nan        nan        nan 0.18196507 0.18262509\n",
      " 0.19113683 0.19178614 0.18327226 0.18327869 0.19113683 0.19243973\n",
      "        nan        nan        nan        nan 0.18261866 0.18262724\n",
      " 0.19244616 0.19243973 0.18261866 0.18262724 0.19244616 0.19243973\n",
      "        nan        nan        nan        nan 0.18065788 0.18066645\n",
      " 0.19309975 0.19113254 0.18065788 0.18066645 0.19309975 0.19113254\n",
      "        nan        nan        nan        nan 0.23039109 0.24020786\n",
      " 0.23235616 0.23693989 0.23039109 0.24020786 0.23235616 0.23693989\n",
      "        nan        nan        nan        nan 0.23169827 0.24020786\n",
      " 0.23235616 0.23824708 0.23169827 0.24020786 0.23235616 0.23824708\n",
      "        nan        nan        nan        nan 0.23170042 0.23955427\n",
      " 0.23235616 0.23759134 0.23170042 0.23955427 0.23235616 0.23759134\n",
      "        nan        nan        nan        nan 0.17148398 0.17410908\n",
      " 0.17608057 0.18066645 0.17148398 0.17410693 0.17608057 0.18132005\n",
      "        nan        nan        nan        nan 0.17148184 0.17542055\n",
      " 0.17607843 0.18394086 0.17148184 0.17542055 0.17607843 0.18394086\n",
      "        nan        nan        nan        nan 0.17017251 0.17345334\n",
      " 0.1786971  0.18066645 0.17017251 0.17345334 0.1786971  0.18066645\n",
      "        nan        nan        nan        nan 0.2572399  0.2552834\n",
      " 0.26641594 0.26838101 0.25724204 0.25593914 0.26641594 0.26903461\n",
      "        nan        nan        nan        nan 0.25788921 0.25528126\n",
      " 0.26575806 0.2703418  0.25788921 0.25528126 0.26575806 0.2703418\n",
      "        nan        nan        nan        nan 0.25657988 0.25659274\n",
      " 0.26314583 0.26903889 0.25657988 0.25659274 0.26314583 0.26903889\n",
      "        nan        nan        nan        nan 0.34820101 0.34492446\n",
      " 0.34295511 0.34164792 0.34820101 0.34492446 0.34295511 0.34164792\n",
      "        nan        nan        nan        nan 0.34754741 0.34427087\n",
      " 0.34033644 0.33903139 0.34754741 0.34427087 0.34033644 0.33903139\n",
      "        nan        nan        nan        nan 0.34689596 0.34296582\n",
      " 0.33902711 0.33771992 0.34689596 0.34296582 0.33902711 0.33771992\n",
      "        nan        nan        nan        nan 0.21337405 0.22123004\n",
      " 0.22646523 0.23563484 0.2140255  0.21992285 0.22711883 0.23759349\n",
      "        nan        nan        nan        nan 0.21010822 0.21861566\n",
      " 0.22777671 0.23759563 0.21010822 0.21861566 0.22777671 0.23759563\n",
      "        nan        nan        nan        nan 0.20748312 0.21860924\n",
      " 0.22777456 0.23825137 0.20748312 0.21860924 0.22777456 0.23825137\n",
      "        nan        nan        nan        nan 0.33706418 0.34099646\n",
      " 0.34099432 0.34099218 0.33706632 0.34165006 0.34361299 0.34230151\n",
      "        nan        nan        nan        nan 0.3396807  0.33771992\n",
      " 0.34164577 0.34229937 0.3396807  0.33771992 0.34164577 0.34229937\n",
      "        nan        nan        nan        nan 0.33706418 0.33576342\n",
      " 0.33706847 0.33968499 0.33706418 0.33576342 0.33706847 0.33968499\n",
      "        nan        nan        nan        nan 0.36391085 0.37307832\n",
      " 0.37307832 0.37700204 0.36653595 0.37308261 0.37308047 0.37831137\n",
      "        nan        nan        nan        nan 0.36587592 0.37242044\n",
      " 0.37373192 0.37765992 0.36587592 0.37242044 0.37373192 0.37765992\n",
      "        nan        nan        nan        nan 0.36653595 0.37896496\n",
      " 0.37569913 0.3756927  0.36653595 0.37896496 0.37569913 0.3756927\n",
      "        nan        nan        nan        nan 0.43588128 0.43457195\n",
      " 0.44046073 0.43522769 0.44832101 0.44176792 0.44242366 0.43915354\n",
      "        nan        nan        nan        nan 0.43716918 0.44240866\n",
      " 0.44568306 0.44503375 0.43716918 0.44240866 0.44568306 0.44503375\n",
      "        nan        nan        nan        nan 0.43065038 0.42933676\n",
      " 0.42868317 0.42672453 0.43065038 0.42933676 0.42868317 0.42672453\n",
      "        nan        nan        nan        nan 0.45223615 0.45288546\n",
      " 0.46270867 0.46532519 0.45747991 0.4555127  0.45617058 0.46139934\n",
      "        nan        nan        nan        nan 0.45812493 0.4627001\n",
      " 0.46597878 0.46008572 0.45812493 0.4627001  0.46597878 0.46008572\n",
      "        nan        nan        nan        nan 0.45943427 0.46009\n",
      " 0.46139719 0.4574692  0.45943427 0.46009    0.46139719 0.4574692\n",
      "        nan        nan        nan        nan 0.4744798  0.46270867\n",
      " 0.47318119 0.46729455 0.46925104 0.47056895 0.47580628 0.47121397\n",
      "        nan        nan        nan        nan 0.46335155 0.47054538\n",
      " 0.47251688 0.47120969 0.46335155 0.47054538 0.47251688 0.47120969\n",
      "        nan        nan        nan        nan 0.46531662 0.45943212\n",
      " 0.46270438 0.46336012 0.46531662 0.45943212 0.46270438 0.46336012]\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\anaconda3.1\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan 0.03722925 0.05342759\n",
      " 0.06449549 0.0753663  0.03722925 0.05342759 0.06449549 0.0753663\n",
      "        nan        nan        nan        nan 0.03722925 0.05342759\n",
      " 0.06449549 0.0753663  0.03722925 0.05342759 0.06449549 0.0753663\n",
      "        nan        nan        nan        nan 0.03848656 0.05342759\n",
      " 0.06449549 0.0753663  0.03848656 0.05342759 0.06449549 0.0753663\n",
      "        nan        nan        nan        nan 0.10263895 0.11801435\n",
      " 0.12915351 0.13040494 0.10263895 0.11801435 0.12915351 0.13040494\n",
      "        nan        nan        nan        nan 0.10263895 0.11801435\n",
      " 0.12915351 0.13040494 0.10263895 0.11801435 0.12915351 0.13040494\n",
      "        nan        nan        nan        nan 0.10146936 0.11801435\n",
      " 0.12915351 0.13040494 0.10146936 0.11801435 0.12915351 0.13040494\n",
      "        nan        nan        nan        nan 0.1386058  0.1735242\n",
      " 0.19165965 0.18722862 0.1386058  0.1735242  0.19165965 0.18722862\n",
      "        nan        nan        nan        nan 0.1386058  0.17347717\n",
      " 0.19162116 0.18614059 0.1386058  0.17347717 0.19162116 0.18614059\n",
      "        nan        nan        nan        nan 0.1386058  0.17347717\n",
      " 0.19266459 0.18718401 0.1386058  0.17347717 0.19266459 0.18718401\n",
      "        nan        nan        nan        nan 0.11562197 0.12018459\n",
      " 0.13943356 0.15058486 0.11562197 0.12018459 0.13943356 0.15058486\n",
      "        nan        nan        nan        nan 0.11562197 0.118993\n",
      " 0.13943356 0.14946795 0.11562197 0.118993   0.13943356 0.14946795\n",
      "        nan        nan        nan        nan 0.1167669  0.12253217\n",
      " 0.14053707 0.15171785 0.1167669  0.12253217 0.14053707 0.15171785\n",
      "        nan        nan        nan        nan 0.30228182 0.30264704\n",
      " 0.3141031  0.31505914 0.30396721 0.30365067 0.3141031  0.31603481\n",
      "        nan        nan        nan        nan 0.30306882 0.30270382\n",
      " 0.31599677 0.31603481 0.30306882 0.30270382 0.31599677 0.31603481\n",
      "        nan        nan        nan        nan 0.30014148 0.29973138\n",
      " 0.31698348 0.31409968 0.30014148 0.29973138 0.31698348 0.31409968\n",
      "        nan        nan        nan        nan 0.36267064 0.37245035\n",
      " 0.36435742 0.3698037  0.36267064 0.37245035 0.36435742 0.3698037\n",
      "        nan        nan        nan        nan 0.36429878 0.37245035\n",
      " 0.36435742 0.37160666 0.36429878 0.37245035 0.36435742 0.37160666\n",
      "        nan        nan        nan        nan 0.36430048 0.37166121\n",
      " 0.36435742 0.37101872 0.36430048 0.37166121 0.36435742 0.37101872\n",
      "        nan        nan        nan        nan 0.28736803 0.2911046\n",
      " 0.29383263 0.30076944 0.28736803 0.29118141 0.29389123 0.30179034\n",
      "        nan        nan        nan        nan 0.28751063 0.29293199\n",
      " 0.29407977 0.3056246  0.28751063 0.29293199 0.29407977 0.3056246\n",
      "        nan        nan        nan        nan 0.28593214 0.29029455\n",
      " 0.29782319 0.30053752 0.28593214 0.29029455 0.29782319 0.30053752\n",
      "        nan        nan        nan        nan 0.39286454 0.38976824\n",
      " 0.40226463 0.40510473 0.39279412 0.39033559 0.40179672 0.40587814\n",
      "        nan        nan        nan        nan 0.39306551 0.39003284\n",
      " 0.40159135 0.40722605 0.39306551 0.39003284 0.40159135 0.40722605\n",
      "        nan        nan        nan        nan 0.39248292 0.39231995\n",
      " 0.39906415 0.40654754 0.39248292 0.39231995 0.39906415 0.40654754\n",
      "        nan        nan        nan        nan 0.48534649 0.48390701\n",
      " 0.48198239 0.48032843 0.48534649 0.48390701 0.48198239 0.4805521\n",
      "        nan        nan        nan        nan 0.48494047 0.48364277\n",
      " 0.47961063 0.47771978 0.48494047 0.48364277 0.47961063 0.47771978\n",
      "        nan        nan        nan        nan 0.48443224 0.48256115\n",
      " 0.47841318 0.47657047 0.48443224 0.48256115 0.47841318 0.47657047\n",
      "        nan        nan        nan        nan 0.34404367 0.35400383\n",
      " 0.36007112 0.37116712 0.34471132 0.35229792 0.36094877 0.37385503\n",
      "        nan        nan        nan        nan 0.3394484  0.35035098\n",
      " 0.36226037 0.37332189 0.3394484  0.35035098 0.36226037 0.37332189\n",
      "        nan        nan        nan        nan 0.33572078 0.35053626\n",
      " 0.36103623 0.37266757 0.33572078 0.35053626 0.36103623 0.37266757\n",
      "        nan        nan        nan        nan 0.47926502 0.48419708\n",
      " 0.48228602 0.48234381 0.47918924 0.4844969  0.48572897 0.48339783\n",
      "        nan        nan        nan        nan 0.48163347 0.4801799\n",
      " 0.48321648 0.48371601 0.48163347 0.4801799  0.48321648 0.48371601\n",
      "        nan        nan        nan        nan 0.47882454 0.47839652\n",
      " 0.4782478  0.48132708 0.47882454 0.47839652 0.4782478  0.48132708\n",
      "        nan        nan        nan        nan 0.50170936 0.50658235\n",
      " 0.50681271 0.5101833  0.50425054 0.50704717 0.50723419 0.51212654\n",
      "        nan        nan        nan        nan 0.50260761 0.50637665\n",
      " 0.50777283 0.51130459 0.50260761 0.50637665 0.50777283 0.51130459\n",
      "        nan        nan        nan        nan 0.50189419 0.51245594\n",
      " 0.50906105 0.50845824 0.50189419 0.51245594 0.50906105 0.50845824\n",
      "        nan        nan        nan        nan 0.55295294 0.55347798\n",
      " 0.55709017 0.55336793 0.56609145 0.56234416 0.56156116 0.55929152\n",
      "        nan        nan        nan        nan 0.55915342 0.56427466\n",
      " 0.56744322 0.56737586 0.55915342 0.56427466 0.56744322 0.56737586\n",
      "        nan        nan        nan        nan 0.55426032 0.55292453\n",
      " 0.55231006 0.5504428  0.55426032 0.55292453 0.55231006 0.5504428\n",
      "        nan        nan        nan        nan 0.55915487 0.5617127\n",
      " 0.57118515 0.57575204 0.56820306 0.56617504 0.56619136 0.57170346\n",
      "        nan        nan        nan        nan 0.56916667 0.5749996\n",
      " 0.57933546 0.57276709 0.56916667 0.5749996  0.57933546 0.57276709\n",
      "        nan        nan        nan        nan 0.57417301 0.5753051\n",
      " 0.5770875  0.57399638 0.57417301 0.5753051  0.5770875  0.57399638\n",
      "        nan        nan        nan        nan 0.57359592 0.56658011\n",
      " 0.57488514 0.57035623 0.57376621 0.57440285 0.57757834 0.57530348\n",
      "        nan        nan        nan        nan 0.57112953 0.57770484\n",
      " 0.58052856 0.57904801 0.57112953 0.57770484 0.58052856 0.57904801\n",
      "        nan        nan        nan        nan 0.57796669 0.57195662\n",
      " 0.57467237 0.5749074  0.57796669 0.57195662 0.57467237 0.5749074 ]\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\anaconda3.1\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan 0.80013333 0.80186667\n",
      " 0.80306667 0.80426667 0.80013333 0.80186667 0.80306667 0.80426667\n",
      "        nan        nan        nan        nan 0.80013333 0.80186667\n",
      " 0.80306667 0.80426667 0.80013333 0.80186667 0.80306667 0.80426667\n",
      "        nan        nan        nan        nan 0.80026667 0.80186667\n",
      " 0.80306667 0.80426667 0.80026667 0.80186667 0.80306667 0.80426667\n",
      "        nan        nan        nan        nan 0.80733333 0.80906667\n",
      " 0.81       0.81026667 0.80733333 0.80906667 0.81       0.81026667\n",
      "        nan        nan        nan        nan 0.80733333 0.80906667\n",
      " 0.81       0.81026667 0.80733333 0.80906667 0.81       0.81026667\n",
      "        nan        nan        nan        nan 0.8072     0.80906667\n",
      " 0.81       0.81026667 0.8072     0.80906667 0.81       0.81026667\n",
      "        nan        nan        nan        nan 0.81133333 0.8148\n",
      " 0.81693333 0.81613333 0.81133333 0.8148     0.81693333 0.81613333\n",
      "        nan        nan        nan        nan 0.81133333 0.8148\n",
      " 0.81693333 0.816      0.81133333 0.8148     0.81693333 0.816\n",
      "        nan        nan        nan        nan 0.81133333 0.8148\n",
      " 0.81706667 0.81613333 0.81133333 0.8148     0.81706667 0.81613333\n",
      "        nan        nan        nan        nan 0.80866667 0.80893333\n",
      " 0.8112     0.81253333 0.80866667 0.80893333 0.8112     0.81253333\n",
      "        nan        nan        nan        nan 0.80866667 0.8088\n",
      " 0.8112     0.8124     0.80866667 0.8088     0.8112     0.8124\n",
      "        nan        nan        nan        nan 0.8088     0.80933333\n",
      " 0.81133333 0.81266667 0.8088     0.80933333 0.81133333 0.81266667\n",
      "        nan        nan        nan        nan 0.82973333 0.82973333\n",
      " 0.83106667 0.8312     0.82973333 0.82986667 0.83106667 0.83133333\n",
      "        nan        nan        nan        nan 0.8296     0.82973333\n",
      " 0.83146667 0.83133333 0.8296     0.82973333 0.83146667 0.83133333\n",
      "        nan        nan        nan        nan 0.8292     0.82933333\n",
      " 0.8316     0.83106667 0.8292     0.82933333 0.8316     0.83106667\n",
      "        nan        nan        nan        nan 0.83533333 0.8356\n",
      " 0.83533333 0.836      0.83533333 0.8356     0.83533333 0.836\n",
      "        nan        nan        nan        nan 0.8356     0.8356\n",
      " 0.83533333 0.8364     0.8356     0.8356     0.83533333 0.8364\n",
      "        nan        nan        nan        nan 0.8356     0.83546667\n",
      " 0.83533333 0.8364     0.8356     0.83546667 0.83533333 0.8364\n",
      "        nan        nan        nan        nan 0.828      0.82853333\n",
      " 0.8292     0.83026667 0.828      0.82853333 0.8292     0.8304\n",
      "        nan        nan        nan        nan 0.82786667 0.82893333\n",
      " 0.8292     0.83093333 0.82786667 0.82893333 0.8292     0.83093333\n",
      "        nan        nan        nan        nan 0.828      0.82853333\n",
      " 0.82973333 0.83       0.828      0.82853333 0.82973333 0.83\n",
      "        nan        nan        nan        nan 0.83893333 0.83813333\n",
      " 0.84       0.8408     0.83893333 0.83813333 0.83973333 0.84093333\n",
      "        nan        nan        nan        nan 0.83866667 0.83826667\n",
      " 0.84       0.84106667 0.83866667 0.83826667 0.84       0.84106667\n",
      "        nan        nan        nan        nan 0.83893333 0.83906667\n",
      " 0.83986667 0.84133333 0.83893333 0.83906667 0.83986667 0.84133333\n",
      "        nan        nan        nan        nan 0.85       0.8504\n",
      " 0.85       0.8496     0.85       0.8504     0.85       0.84973333\n",
      "        nan        nan        nan        nan 0.85       0.85053333\n",
      " 0.84973333 0.8492     0.85       0.85053333 0.84973333 0.8492\n",
      "        nan        nan        nan        nan 0.85       0.85053333\n",
      " 0.8496     0.84906667 0.85       0.85053333 0.8496     0.84906667\n",
      "        nan        nan        nan        nan 0.8348     0.836\n",
      " 0.8364     0.83786667 0.83466667 0.83573333 0.83653333 0.83826667\n",
      "        nan        nan        nan        nan 0.834      0.83546667\n",
      " 0.83706667 0.83813333 0.834      0.83546667 0.83706667 0.83813333\n",
      "        nan        nan        nan        nan 0.8332     0.83546667\n",
      " 0.83626667 0.8372     0.8332     0.83546667 0.83626667 0.8372\n",
      "        nan        nan        nan        nan 0.85093333 0.85226667\n",
      " 0.85106667 0.85106667 0.85093333 0.85213333 0.852      0.8512\n",
      "        nan        nan        nan        nan 0.8512     0.8512\n",
      " 0.85133333 0.85133333 0.8512     0.8512     0.85133333 0.85133333\n",
      "        nan        nan        nan        nan 0.85066667 0.85106667\n",
      " 0.8504     0.85106667 0.85066667 0.85106667 0.8504     0.85106667\n",
      "        nan        nan        nan        nan 0.85293333 0.85213333\n",
      " 0.85253333 0.85293333 0.85346667 0.85253333 0.8528     0.8536\n",
      "        nan        nan        nan        nan 0.85266667 0.85226667\n",
      " 0.85266667 0.8532     0.85266667 0.85226667 0.85266667 0.8532\n",
      "        nan        nan        nan        nan 0.85213333 0.85346667\n",
      " 0.85293333 0.85253333 0.85213333 0.85346667 0.85293333 0.85253333\n",
      "        nan        nan        nan        nan 0.85666667 0.85746667\n",
      " 0.8576     0.85706667 0.86013333 0.86013333 0.8596     0.85933333\n",
      "        nan        nan        nan        nan 0.8596     0.86093333\n",
      " 0.86173333 0.86186667 0.8596     0.86093333 0.86173333 0.86186667\n",
      "        nan        nan        nan        nan 0.85906667 0.8588\n",
      " 0.85866667 0.85826667 0.85906667 0.8588     0.85866667 0.85826667\n",
      "        nan        nan        nan        nan 0.85506667 0.85626667\n",
      " 0.85866667 0.86053333 0.85866667 0.85813333 0.85786667 0.85946667\n",
      "        nan        nan        nan        nan 0.8588     0.8608\n",
      " 0.8624     0.8604     0.8588     0.8608     0.8624     0.8604\n",
      "        nan        nan        nan        nan 0.8616     0.862\n",
      " 0.86253333 0.86186667 0.8616     0.862      0.86253333 0.86186667\n",
      "        nan        nan        nan        nan 0.85666667 0.856\n",
      " 0.8576     0.8568     0.85813333 0.85813333 0.85853333 0.85853333\n",
      "        nan        nan        nan        nan 0.85866667 0.8604\n",
      " 0.86133333 0.86093333 0.85866667 0.8604     0.86133333 0.86093333\n",
      "        nan        nan        nan        nan 0.86173333 0.86\n",
      " 0.86066667 0.86066667 0.86173333 0.86       0.86066667 0.86066667]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_depth': None,\n",
       " 'max_features': 4,\n",
       " 'min_samples_leaf': 2,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 125}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_cv.fit(X_train, y_train)\n",
    "rf_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.580528563620339"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_results(model_name, model_object):\n",
    "\n",
    "    cv_results = pd.DataFrame(model_object.cv_results_)\n",
    "\n",
    "    best_estimator_results = cv_results.iloc[cv_results['mean_test_f1'].idxmax(), :]\n",
    "\n",
    "    # Extract accuracy, precision, recall, and f1 score from that row.\n",
    "    f1 = best_estimator_results.mean_test_f1\n",
    "    recall = best_estimator_results.mean_test_recall\n",
    "    precision = best_estimator_results.mean_test_precision\n",
    "    accuracy = best_estimator_results.mean_test_accuracy\n",
    "\n",
    " \n",
    "    # Create table of results\n",
    "    table = pd.DataFrame({'Model': [model_name],\n",
    "                          'F1': [f1],\n",
    "                          'Recall': [recall],\n",
    "                          'Precision': [precision],\n",
    "                          'Accuracy': [accuracy]\n",
    "                         }\n",
    "                        )\n",
    "\n",
    "    return table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest CV</td>\n",
       "      <td>0.580529</td>\n",
       "      <td>0.472517</td>\n",
       "      <td>0.756289</td>\n",
       "      <td>0.861333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Model        F1    Recall  Precision  Accuracy\n",
       "0  Random Forest CV  0.580529  0.472517   0.756289  0.861333"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_cv_results = make_results('Random Forest CV', rf_cv)\n",
    "rf_cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cv_results.to_csv(path+'results1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tuned Decision Tree</td>\n",
       "      <td>0.560655</td>\n",
       "      <td>0.469255</td>\n",
       "      <td>0.701608</td>\n",
       "      <td>0.8504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model        F1    Recall  Precision  Accuracy\n",
       "0  Tuned Decision Tree  0.560655  0.469255   0.701608    0.8504"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv('results1.csv', index_col=0)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.concat([rf_cv_results, results], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest CV</td>\n",
       "      <td>0.580529</td>\n",
       "      <td>0.472517</td>\n",
       "      <td>0.756289</td>\n",
       "      <td>0.861333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tuned Decision Tree</td>\n",
       "      <td>0.560655</td>\n",
       "      <td>0.469255</td>\n",
       "      <td>0.701608</td>\n",
       "      <td>0.850400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model        F1    Recall  Precision  Accuracy\n",
       "0     Random Forest CV  0.580529  0.472517   0.756289  0.861333\n",
       "0  Tuned Decision Tree  0.560655  0.469255   0.701608  0.850400"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.20, stratify=y_train, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = [0 if x in X_val.index else -1 for x in X_train.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import PredefinedSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=0)\n",
    "cv_params = {'max_depth':[2,3,4,5, None],\n",
    "             'min_samples_leaf':[1,2,3],\n",
    "             'min_samples_split':[2,3,4],\n",
    "             'max_features':[2,3,4],\n",
    "             'n_estimators':[75, 100, 125, 150]}\n",
    "scoring = {'accuracy', 'precision', 'recall', 'f1'}\n",
    "custom_split = PredefinedSplit(split_index)\n",
    "rf_val = GridSearchCV(rf, param_grid=cv_params, scoring=scoring, cv = custom_split,refit='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=PredefinedSplit(test_fold=array([-1,  0, ..., -1, -1])),\n",
       "             estimator=RandomForestClassifier(random_state=0),\n",
       "             param_grid={&#x27;max_depth&#x27;: [2, 3, 4, 5, None],\n",
       "                         &#x27;max_features&#x27;: [2, 3, 4],\n",
       "                         &#x27;min_samples_leaf&#x27;: [1, 2, 3],\n",
       "                         &#x27;min_samples_split&#x27;: [2, 3, 4],\n",
       "                         &#x27;n_estimators&#x27;: [75, 100, 125, 150]},\n",
       "             refit=&#x27;f1&#x27;, scoring={&#x27;precision&#x27;, &#x27;recall&#x27;, &#x27;f1&#x27;, &#x27;accuracy&#x27;})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=PredefinedSplit(test_fold=array([-1,  0, ..., -1, -1])),\n",
       "             estimator=RandomForestClassifier(random_state=0),\n",
       "             param_grid={&#x27;max_depth&#x27;: [2, 3, 4, 5, None],\n",
       "                         &#x27;max_features&#x27;: [2, 3, 4],\n",
       "                         &#x27;min_samples_leaf&#x27;: [1, 2, 3],\n",
       "                         &#x27;min_samples_split&#x27;: [2, 3, 4],\n",
       "                         &#x27;n_estimators&#x27;: [75, 100, 125, 150]},\n",
       "             refit=&#x27;f1&#x27;, scoring={&#x27;precision&#x27;, &#x27;recall&#x27;, &#x27;f1&#x27;, &#x27;accuracy&#x27;})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=0)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=0)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1,  0, ..., -1, -1])),\n",
       "             estimator=RandomForestClassifier(random_state=0),\n",
       "             param_grid={'max_depth': [2, 3, 4, 5, None],\n",
       "                         'max_features': [2, 3, 4],\n",
       "                         'min_samples_leaf': [1, 2, 3],\n",
       "                         'min_samples_split': [2, 3, 4],\n",
       "                         'n_estimators': [75, 100, 125, 150]},\n",
       "             refit='f1', scoring={'precision', 'recall', 'f1', 'accuracy'})"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_val.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (path+'rf_val_model.pickle', 'wb') as to_write:\n",
    "    pickle.dump(rf_val, to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+'rf_val_model.pickle', 'rb') as to_read:\n",
    "    rf_val = pickle.load(to_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': None,\n",
       " 'max_features': 4,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 3,\n",
       " 'n_estimators': 150}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_val.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest CV</td>\n",
       "      <td>0.580529</td>\n",
       "      <td>0.472517</td>\n",
       "      <td>0.756289</td>\n",
       "      <td>0.861333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest CV</td>\n",
       "      <td>0.580529</td>\n",
       "      <td>0.472517</td>\n",
       "      <td>0.756289</td>\n",
       "      <td>0.861333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tuned Decision Tree</td>\n",
       "      <td>0.560655</td>\n",
       "      <td>0.469255</td>\n",
       "      <td>0.701608</td>\n",
       "      <td>0.850400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model        F1    Recall  Precision  Accuracy\n",
       "0     Random Forest CV  0.580529  0.472517   0.756289  0.861333\n",
       "0     Random Forest CV  0.580529  0.472517   0.756289  0.861333\n",
       "0  Tuned Decision Tree  0.560655  0.469255   0.701608  0.850400"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_val_results = make_results('Random Forest Validated', rf_val)\n",
    "results = pd.concat([rf_cv_results, results], axis=0)\n",
    "\n",
    "results.sort_values(by=['F1'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(path+'results2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
